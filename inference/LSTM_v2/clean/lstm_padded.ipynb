{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7449,"status":"ok","timestamp":1711781225264,"user":{"displayName":"Jan Rhey Lagaña","userId":"10820671969604914994"},"user_tz":-480},"id":"Vwx4g5bE-fBq"},"outputs":[],"source":["import sys\n","import copy\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from torch.nn.utils.rnn import pad_sequence\n","from sklearn.metrics import confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def count(my_array):\n","    # Initialize the count\n","    norm_count = 0\n","    ano_count = 0\n","\n","    # Iterate through each element in the list\n","    for id in my_array:\n","        if 'normal' in id:\n","            norm_count += 1\n","        elif 'anomaly' in id:\n","            ano_count += 1\n","\n","    return (ano_count, norm_count)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1711781229130,"user":{"displayName":"Jan Rhey Lagaña","userId":"10820671969604914994"},"user_tz":-480},"id":"03fJ1Ifz_Xx5"},"outputs":[{"name":"stdout","output_type":"stream","text":["training (anomaly, normal):  (47, 51)\n","validation (anomaly, normal):  (13, 8)\n","testing (anomaly, normal):  (10, 11)\n"]}],"source":["# n_features = 7  # Number of features\n","# sequence_length = 20  # Length of each sequence\n","\n","df = pd.read_csv('data.csv')\n","video_ids = df['video_id'].unique()\n","\n","# Split video IDs into train, validation, and test sets\n","train_ids, temp_ids = train_test_split(video_ids, test_size=0.3, random_state=42)\n","val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n","\n","train_ano_norm = count(train_ids)\n","val_ano_norm = count(val_ids)\n","test_ano_norm = count(test_ids)\n","\n","print('training (anomaly, normal): ', train_ano_norm)\n","print('validation (anomaly, normal): ', val_ano_norm)\n","print('testing (anomaly, normal): ', test_ano_norm)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'numpy.ndarray' object has no attribute 'extend'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     16\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Reshape features into a single sequence\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# features = torch.tensor(features).unsqueeze(0)  # Add batch dimension\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Append to X_train\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m(features)\n\u001b[0;32m     26\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_test)\n","\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'extend'"]}],"source":["X_train = []\n","y_train = []\n","\n","X_val = []\n","y_val = []\n","\n","X_test = []\n","y_test = []\n","\n","\n","# Process train, validation, and test sets\n","for test_id in test_ids[:2]:\n","    filtered_df = df[df['video_id'] == test_id]\n","    features = filtered_df[['frame_num','crowd_density', 'loitering', 'no_concealment', 'low_concealment', 'med_concealment', 'high_concealment']]\n","    RBP = filtered_df['rbp'].iloc[0]\n","    features = features.to_numpy()\n","\n","    # Reshape features into a single sequence\n","    # features = torch.tensor(features).unsqueeze(0)  # Add batch dimension\n","\n","    # Pad the sequence\n","    # padded_features = pad_sequence(features, batch_first=True)\n","\n","    # Append to X_train\n","    X_test.extend(features)\n","    X_test = np.array(X_test)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["[tensor([[[  1.0000,   0.0000,  -1.0000,  ...,   0.0000,   0.0000,   0.0000],\n","          [  2.0000,   2.5245,   0.0000,  ...,   0.0000,   0.0000,   1.0000],\n","          [  3.0000,   2.6224,   0.0000,  ...,   0.0000,   0.0000,   1.0000],\n","          ...,\n","          [314.0000,   6.9848, 124.5427,  ...,   1.0000,   0.0000,   1.0000],\n","          [315.0000,   6.9296, 124.9462,  ...,   1.0000,   0.0000,   1.0000],\n","          [316.0000,   7.0662, 125.3502,  ...,   1.0000,   0.0000,   1.0000]]],\n","        dtype=torch.float64),\n"," tensor([[[  1.0000,   0.0000,  -1.0000,  ...,   0.0000,   0.0000,   0.0000],\n","          [  2.0000,   0.0000,  -1.0000,  ...,   0.0000,   0.0000,   0.0000],\n","          [  3.0000,   0.0000,  -1.0000,  ...,   0.0000,   0.0000,   0.0000],\n","          ...,\n","          [249.0000,   0.3409,   8.0000,  ...,   0.0000,   0.0000,   1.0000],\n","          [250.0000,   0.3436,   8.5000,  ...,   0.0000,   0.0000,   1.0000],\n","          [251.0000,   0.3482,   9.0000,  ...,   0.0000,   0.0000,   1.0000]]],\n","        dtype=torch.float64)]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["X_test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for train_id in train_ids:\n","    filtered_df = df[df['video_id'] == train_id]\n","    features = filtered_df[['crowd_density', 'loitering', 'no_concealment', 'low_concealment', 'med_concealment', 'high_concealment']]\n","    RBP = filtered_df['rbp'].iloc[0]\n","    features = features.to_numpy()\n","    # n_seq = len(features) // sequence_length\n","\n","    # Reshape the features into batches\n","    features = features[:sequence_length * n_seq].reshape(-1, sequence_length, n_features) # one video is one batch\n","    labels = [RBP] * n_seq\n","\n","    # Extend the lists\n","    X_train.extend(features)\n","    y_train.extend(labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","for val_id in val_ids:\n","    filtered_df = df[df['video_id'] == val_id]\n","    features = filtered_df[['crowd_density', 'loitering', 'no_concealment', 'low_concealment', 'med_concealment', 'high_concealment']]\n","    RBP = filtered_df['rbp'].iloc[0]\n","    features = features.to_numpy()\n","    n_seq = len(features) // sequence_length\n","\n","    # Reshape the features into batches\n","    features = features[:sequence_length * n_seq].reshape(-1, sequence_length, n_features) # one video is one batch\n","    labels = [RBP] * n_seq\n","\n","    # Extend the lists\n","    X_val.extend(features)\n","    y_val.extend(labels)\n","\n","for test_id in test_ids:\n","    filtered_df = df[df['video_id'] == test_id]\n","    features = filtered_df[['crowd_density', 'loitering', 'no_concealment', 'low_concealment', 'med_concealment', 'high_concealment']]\n","    RBP = filtered_df['rbp'].iloc[0]\n","    features = features.to_numpy()\n","    n_seq = len(features) // sequence_length\n","\n","    # Reshape the features into batches\n","    features = features[:sequence_length * n_seq].reshape(-1, sequence_length, n_features) # one video is one batch\n","    labels = [RBP] * n_seq\n","\n","    # Extend the lists\n","    X_test.extend(features)\n","    y_test.extend(labels)\n","\n","# Convert to NumPy arrays\n","X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","\n","X_val = np.array(X_val)\n","y_val = np.array(y_val)\n","\n","X_test = np.array(X_test)\n","y_test = np.array(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711781232513,"user":{"displayName":"Jan Rhey Lagaña","userId":"10820671969604914994"},"user_tz":-480},"id":"_JIe90qW_T1a"},"outputs":[],"source":["# Normalize features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n","X_test_scaled = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711781232514,"user":{"displayName":"Jan Rhey Lagaña","userId":"10820671969604914994"},"user_tz":-480},"id":"X5eDa7lG_cny"},"outputs":[],"source":["# Convert data to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n","X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n","y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(LSTMModel, self).__init__()\n","        self.lstm1 = nn.LSTM(input_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, 1)  # Single output neuron for binary classification\n","        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n","\n","    def forward(self, x):\n","        out, _ = self.lstm1(x)\n","        out = self.fc(out[:, -1])  # Use the last hidden state for prediction (summary)\n","        out = self.sigmoid(out) # Apply sigmoid activation\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# class LSTMModel(nn.Module):\n","#     def __init__(self, input_size, hidden_size):\n","#         super(LSTMModel, self).__init__()\n","#         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=False, stateful=True)\n","#         self.fc = nn.Linear(hidden_size, 1)  # Single output neuron for binary classification\n","#         self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n","\n","#     def forward(self, x):\n","#         out, _ = self.lstm(x.unsqueeze(0))  # Add batch dimension (batch_size=1)\n","#         out = self.fc(out[:, -1, :])  # Use the last hidden state for prediction\n","#         out = self.sigmoid(out)  # Apply sigmoid activation\n","#         return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# class LSTMModel(nn.Module):\n","#     def __init__(self, input_size, hidden_size):\n","#         super(LSTMModel, self).__init__()\n","#         self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n","#         self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)  # Additional LSTM layer\n","#         self.fc1 = nn.Linear(hidden_size, 64)  # First dense layer\n","#         self.fc2 = nn.Linear(64, 32)  # Second dense layer\n","#         self.fc3 = nn.Linear(32, 1)  # Single output neuron\n","#         self.fc4 = nn.Sigmoid()  # Sigmoid activation for binary classification\n","\n","#     def forward(self, x):\n","#         out, (h_n, c_n) = self.lstm1(x)\n","#         out, _ = self.lstm2(out, (h_n, c_n))  # Pass through the second LSTM layer\n","#         out = self.fc1(out[:, -1, :])\n","#         out = self.fc2(out)\n","#         out = self.fc3(out)\n","#         out = self.fc4(out)\n","#         return out"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6041,"status":"ok","timestamp":1711781241254,"user":{"displayName":"Jan Rhey Lagaña","userId":"10820671969604914994"},"user_tz":-480},"id":"J3lqeR3_Bljc"},"outputs":[],"source":["# Instantiate the model\n","input_size = n_features\n","hidden_size = 64\n","model = LSTMModel(input_size, hidden_size)\n","\n","# Define loss function and optimizer\n","# criterion = nn.MSELoss()-------------------------------------------\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize empty lists to store training and validation losses\n","train_losses = []\n","val_losses = []\n","val_loss_temp = float('inf')\n","\n","# Training loop\n","n_epochs = 300\n","patience = og_patience = 50\n","for epoch in range(1, n_epochs+1):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor)\n","    loss = criterion(outputs, y_train_tensor.unsqueeze(1))\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    with torch.no_grad():\n","        val_outputs = model(X_val_tensor)\n","        val_loss = criterion(val_outputs, y_val_tensor.unsqueeze(1))\n","\n","    # Append losses to lists\n","    train_losses.append(loss.item())\n","    val_losses.append(val_loss.item())\n","\n","    # save best model\n","    if val_loss.item() < val_loss_temp:\n","        best_model, best_epoch = copy.deepcopy(model), epoch\n","        val_loss_temp = val_loss.item()\n","        patience = 51\n","\n","    patience -= 1\n","    if patience == 0:\n","        print(f\"\\nNo progress in the last {og_patience} epochs. Training completed.\")\n","        break\n","\n","    # print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n","    # Training progress bar\n","    label = f\"{epoch}/{n_epochs} epochs\"\n","    progress = (epoch)/ n_epochs\n","    progress_bar_len = 50\n","    filled_len = int(progress_bar_len * progress)\n","    bar = '=' * filled_len + '-' * (progress_bar_len - filled_len)\n","    sys.stdout.write(f'\\rTraining in progress: [{bar}] {progress * 100:.1f}% [{label}]')\n","    sys.stdout.flush()\n","\n","# Plot the losses\n","plt.figure(figsize=(8, 6))\n","plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n","plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Training and Validation Losses\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = best_model\n","print(f\"Best model at epoch {best_epoch}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1711781241766,"user":{"displayName":"Jan Rhey Lagaña","userId":"10820671969604914994"},"user_tz":-480},"id":"fJQKe1N9B1Y8","outputId":"bc401f36-8d9d-47bb-a07c-dfbb34eeda77"},"outputs":[],"source":["# Evaluate on test set\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test_tensor)\n","\n","    test_loss = criterion(test_outputs, y_test_tensor.unsqueeze(1))\n","    print(f\"Test Loss: {test_loss.item():.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluation\n","precision, recall, thresholds = precision_recall_curve(y_test_tensor, test_outputs)\n","\n","# Compute F1 score (avoid NaN)\n","f1_scores = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(precision), where=(precision + recall) != 0)\n","\n","# Find the optimal threshold that maximizes the F1 score\n","optimal_threshold = thresholds[f1_scores.argmax()]\n","max_f1_score = f1_scores.max()\n","\n","print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n","print(f\"Max F1 Score: {max_f1_score:.4f}\")\n","\n","# Plot the F1 curve and the vertical line for the optimal threshold\n","plt.figure(figsize=(12, 6))\n","\n","# F1 curve\n","plt.subplot(1, 2, 1)\n","plt.plot(thresholds, f1_scores[:-1], label='F1 Score')\n","plt.axvline(x=optimal_threshold, color='r', linestyle='--', label='Optimal Threshold')\n","plt.xlabel('Threshold')\n","plt.ylabel('F1 Score')\n","plt.title('F1 Curve')\n","plt.grid(True)\n","plt.legend()\n","\n","# Confusion matrix\n","plt.subplot(1, 2, 2)\n","test_outputs = (test_outputs >= optimal_threshold).float()\n","cm = confusion_matrix(y_test_tensor, test_outputs)\n","labels = ['No Robbery', 'Robbery']\n","cm_transposed = cm.T\n","\n","sns.heatmap(cm_transposed, annot=True, fmt='d', cmap='Blues', cbar=False,\n","            xticklabels=labels, yticklabels=labels)\n","plt.title('Confusion Matrix')\n","plt.xlabel('True Labels')\n","plt.ylabel('Predicted Labels')\n","\n","plt.tight_layout()  # Adjust spacing between subplots\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":811,"status":"ok","timestamp":1711781253468,"user":{"displayName":"Jan Rhey Lagaña","userId":"10820671969604914994"},"user_tz":-480},"id":"_rkNTTRjB3YY"},"outputs":[],"source":["# Save the model\n","# torch.save(model.state_dict(), f'model_{optimal_threshold:.4f}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(int(len(features))):\n","\n","    input_data = X_train[i]\n","\n","    # Preprocess data\n","    input_data = scaler.fit_transform(input_data.reshape(-1, n_features)).reshape(-1, sequence_length, n_features)[0]\n","    input_data = input_data.tolist()\n","    input_data = torch.tensor(input_data, dtype=torch.float32)\n","\n","    # Ensure input shape matches\n","    if input_data.shape != (sequence_length, n_features):\n","        print('error: ', f'Input shape must be {(sequence_length, n_features)}. Yours has shape {input_data.shape}')\n","\n","    # Make predictions\n","    with torch.no_grad():\n","        output = model(input_data.unsqueeze(0))  # Add batch dimension\n","        RBP = (output).squeeze().cpu().numpy()\n","\n","    actualRBP = y_train[i]\n","\n","    print(f\"\\nprediction: {RBP:.2f} | actual: {actualRBP}\", end=\" \")\n","    if ((RBP >= optimal_threshold) & (actualRBP == 1)) | ((RBP < optimal_threshold) & (actualRBP == 0)):\n","        print(\"| CORRECT\", end=\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# IF NEEDED\n","\n","# # Plot the precision-recall curve\n","# plt.figure(figsize=(8, 6))\n","# plt.plot(recall, precision, label='Precision-Recall Curve')\n","# plt.xlabel('Recall')\n","# plt.ylabel('Precision')\n","# plt.title('Precision-Recall Curve')\n","# plt.grid(True)\n","# plt.legend()\n","# plt.show()\n","\n","# # Plot the F1 curve\n","# plt.figure(figsize=(8, 6))\n","# plt.plot(thresholds, f1_scores[:-1], label='F1 Score')\n","# plt.axvline(x=optimal_threshold, color='r', linestyle='--', label='Optimal Threshold')\n","# plt.xlabel('Threshold')\n","# plt.ylabel('F1 Score')\n","# plt.title('F1 Curve')\n","# plt.grid(True)\n","# plt.legend()\n","# plt.show()\n","\n","# # Plot the Confusion matrix\n","# test_outputs = (test_outputs >= optimal_threshold).float()  # Convert probabilities to 0 or 1\n","# # test_outputs.squeeze(-1)\n","\n","# cm = confusion_matrix(y_test_tensor, test_outputs)\n","# labels = ['No Robbery', 'Robbery']\n","\n","# # Transpose the confusion matrix and swap axis labels\n","# cm_transposed = cm.T\n","\n","# plt.figure(figsize=(8, 6))\n","# sns.heatmap(cm_transposed, annot=True, fmt='d', cmap='Blues', cbar=False,\n","#             xticklabels=labels, yticklabels=labels)  # Set labels for x and y axes\n","# plt.title('Confusion Matrix')\n","# plt.xlabel('True Labels')\n","# plt.ylabel('Predicted Labels')\n","# plt.show()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPMU7dohBYF5DlsgRyJrE/O","mount_file_id":"1_vcn38GyssYpiQMLJxkUNLTHQ0DQiy4R","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
