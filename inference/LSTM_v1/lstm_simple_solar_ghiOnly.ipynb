{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7449,"status":"ok","timestamp":1711781225264,"user":{"displayName":"Jan Rhey LagaÃ±a","userId":"10820671969604914994"},"user_tz":-480},"id":"Vwx4g5bE-fBq"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Load data from solcast.csv\n","ghi_data = pd.read_csv('solcast.csv')\n","\n","n_features = 1\n","sequence_length = 24\n","\n","# Split data into train and validation sets\n","train_size = int(0.8 * len(input_sequences))\n","train_data = input_sequences[:train_size]\n","val_data = input_sequences[train_size:]\n","\n","# Normalize features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.reshape(-1, n_features)).reshape(-1, sequence_length, n_features)\n","X_val_scaled = scaler.transform(X_val.reshape(-1, n_features)).reshape(-1, sequence_length, n_features)\n","X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(-1, sequence_length, n_features)\n","\n","# Create input sequences (e.g., using a sliding window)\n","def create_sequences(data, seq_length):\n","    sequences = []\n","    for i in range(len(data) - seq_length):\n","        seq = data[i : i + seq_length]\n","        sequences.append(seq)\n","    return np.array(sequences)\n","\n","seq_length = 10  # Adjust as needed\n","input_sequences = create_sequences(ghi_data[\"normalized_ghi\"].values, seq_length)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(LSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Define model parameters\n","input_size = 1  # Number of features (GHI values)\n","hidden_size = 64\n","num_layers = 2\n","output_size = 1  # Predict one value (next GHI)\n","model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"LSTM: Expected input to be 2D or 3D, got 1D instead","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(seq, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, seq[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[1;32mc:\\Users\\janrh\\OneDrive - University of the Philippines\\Acads\\4TH YEAR (23-24)\\2ND SEM\\EE 199\\eee-capstone\\env3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\janrh\\OneDrive - University of the Philippines\\Acads\\4TH YEAR (23-24)\\2ND SEM\\EE 199\\eee-capstone\\env3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     11\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 12\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[1;32mc:\\Users\\janrh\\OneDrive - University of the Philippines\\Acads\\4TH YEAR (23-24)\\2ND SEM\\EE 199\\eee-capstone\\env3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\janrh\\OneDrive - University of the Philippines\\Acads\\4TH YEAR (23-24)\\2ND SEM\\EE 199\\eee-capstone\\env3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\janrh\\OneDrive - University of the Philippines\\Acads\\4TH YEAR (23-24)\\2ND SEM\\EE 199\\eee-capstone\\env3.10\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:844\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m--> 844\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    845\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    846\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n","\u001b[1;31mValueError\u001b[0m: LSTM: Expected input to be 2D or 3D, got 1D instead"]}],"source":["criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Split data into train and validation sets\n","train_size = int(0.8 * len(input_sequences))\n","train_data = input_sequences[:train_size]\n","val_data = input_sequences[train_size:]\n","\n","# Train the model\n","num_epochs = 50\n","for epoch in range(num_epochs):\n","    model.train()\n","    for seq in train_data:\n","        optimizer.zero_grad()\n","        seq = torch.tensor(seq, dtype=torch.float32)\n","        output = model(seq)\n","        loss = criterion(output, seq[-1])\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation loss\n","    model.eval()\n","    with torch.no_grad():\n","        val_loss = sum(criterion(model(torch.tensor(seq, dtype=torch.float32)), seq[-1]) for seq in val_data)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"oB4e6TaGK0XE"},"source":["---------------------**Activate API Endpoint** before proceeding-------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lae_LOWWJAXz"},"outputs":[],"source":["import requests\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# input_data = {\n","#     \"input\": [\n","#         [0.1, 0.2, 0.3],\n","#         [0.4, 0.5, 0.6],\n","#         [0.1, 0.2, 0.3],\n","#         [0.4, 0.5, 0.6],\n","#         [0.1, 0.2, 0.3],\n","#         [0.4, 0.5, 0.6],\n","#         [0.1, 0.2, 0.3],\n","#         [0.4, 0.5, 0.6],\n","#         [0.1, 0.2, 0.3],\n","#         [0.4, 0.5, 0.6],\n","        \n","#     ]\n","# }\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# response = requests.post('http://localhost:5000/predict', json=input_data).json()\n","# # print(response)\n","\n","# threshold = 0.5\n","# RBP = response[\"output\"]\n","\n","# print(\"RBP =\", RBP)\n","# if RBP >= threshold:\n","#     print(\"WARNING! Impending robbery.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(int(n_samples*0.15)):\n","    input_data = {\n","    \"input\":  X_test_scaled.tolist()[i] \n","}\n","    response = requests.post('http://localhost:5000/predict', json=input_data).json()\n","\n","    threshold = 0.5\n","    RBP = response[\"output\"]\n","\n","    print(i, \"RBP =\", RBP, \"| actual RBP =\", y_test.tolist()[i])\n","    if RBP >= threshold:\n","        print(\"WARNING! Impending robbery.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# study first doen lstm\n","# work on dynamic input\n","# try solar dataset"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPMU7dohBYF5DlsgRyJrE/O","mount_file_id":"1_vcn38GyssYpiQMLJxkUNLTHQ0DQiy4R","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
